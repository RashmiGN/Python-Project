#!/usr/bin/env python
# coding: utf-8

# In[2]:


import pandas as pd
import threading
import multiprocessing
from parsel import Selector
from time import sleep
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from threading import Lock

opts = Options()

driver = webdriver.Chrome(options=opts, executable_path=r'C:\\Users\\dell\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe')

def validate_field(field):
    if field:
        pass
    else:
        field='No results'
    return field

driver.get('https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin')
username = driver.find_element(By.ID,'username')
username.send_keys('rashmignaik231@gmail.com')
sleep(0.5)
password = driver.find_element(By.ID,'password')
password.send_keys('Rashmi@23')
sleep(0.5)
sign_in_button = driver.find_element(By.XPATH,'//*[@type="submit"]')
sign_in_button.click()
sleep(20)

profile_data = []
links =[]

# href_skill = [element.get_attribute("href") for element in driver.find_elements(By.XPATH, "//div[@class='pvs-list__footer-wrapper']//a[contains(@id,'skills')]")]
# href_exp = [element.get_attribute("href") for element in driver.find_elements(By.XPATH, "//div[@class='pvs-list__footer-wrapper']//a[contains(@id,'experiences')]")]

def education(prof_link):
    driver.get(prof_link)
    sleep(6)
    sel= Selector(text=driver.page_source)
    
    try:
        edu = sel.xpath('//span[contains(@class, "t-14 t-normal")]/span[contains(@aria-hidden,"true")]/text()')
    except:
        edu = 'None'
    edu_list =[]
    for e in edu:
        edu_list.append(str(e))

    educ = []

    for e in edu_list:
        if "Master" in e or "Bachelor" in e or "PhD" in e or "Data" in e:
            educ.append(e[109:-2])
        else:
            None
    
    # print(education_list)
    return educ

def experience(prof_link,href_exp=[None]):
    sel= Selector(text=driver.page_source)
    if href_exp:
        for e in href_exp:
            driver.get(e)
            sleep(3)
            exp = sel.xpath('//span[contains(@class,"t-14 t-normal t-black--light")]/span[contains(@aria-hidden,"true")]/text()')
    else:
        exp = sel.xpath('//span[contains(@class,"t-14 t-normal t-black--light")]/span[contains(@aria-hidden,"true")]/text()')
        
    exp_temp =[]
    for j in exp:
        exp_temp.append(str(j).split('Â·'))
    experience_list = []
    experiences =[]
    for x in range(0,len(exp_temp),2):
        if len(exp_temp[x])>1:
            experience_list.append(exp_temp[x][1][0:-2])
    cum_exp = 0
    for m in experience_list:
        experiences =[int(x) for x in m.split(" ") if x.isnumeric()]
        if len(experiences)>1:
            cum_exp += 12*experiences[0]+experiences[1]
        elif "yr" in experience_list:
            cum_exp += 12*experiences[0]
        else:
            cum_exp += experiences[0]
    
    return cum_exp        
    
#     for m in experience_list:
#     if "yr" in m or "mo" in m:
#         if len(m)==12 or len(m)==13 or len(m)==14:
#             year=int(m[0:2])*12+int(m[6:8])
#         elif "yr" in m:
#             if len(m)==5 or len(m)==6:
#                 year=int(m[0:2])*12
#         else:
#             year = int(m[0:2])
        
#         experiences.append(year)
# #         experiences= sum(int(experiences))
#     else:
#         None
# experiences = sum(experiences)
                   
#     # print(experiences)
#     return experiences


def skills(prof_link,href_skill=[None]):
    skills_list = []
    if href_skill:
        for skill in href_skill:
            driver.get(skill)
            sleep(4)
            sel= Selector(text=driver.page_source)
            whole_skills_data = sel.xpath('//div[contains(@class,"mr1 hoverable-link-text t-bold")]/span[contains(@aria-hidden,"true")]/text()')
            for i in whole_skills_data:
                skills_list.append(str(i)[124:-2])
    else:
        skills_list =[]
    
    
    # print(skills_list)
    return list(set(skills_list))

for x in range(0, 300, 10):
    url = f'https://www.google.com/search?q=site%3Alinkedin.com%2Fin%2F+AND+%22Data+Science%22&sca_esv=563362597&sxsrf=AB5stBjKK7gu1qTnN55UNLW7ellGahD6ig%3A1694083766333&ei=tqr5ZLz1E56Qur8P_9-fuAI&ved=0ahUKEwi8qPLoqZiBAxUeiO4BHf_vBycQ4dUDCA8&oq=site%3Alinkedin.com%2Fin%2F+AND+%22Data+Science%22&gs_lp=Egxnd3Mtd2l6LXNlcnAiKHNpdGU6bGlua2VkaW4uY29tL2luLyBBTkQgIkRhdGEgU2NpZW5jZSJIp2dQ4DVYl1lwBHgAkAEAmAHOAaABqROqAQYwLjEzLjG4AQzIAQD4AQHiAwQYASBBiAYB&sclient=gws-wiz-serp&start={x}'
    driver.get(url)
    # Wait for the search results to load
    WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.XPATH, "//div[@class='yuRUbf']//a")))
    # Scrape the LinkedIn URLs
    linkedin_urls = [my_element.get_attribute("href") for my_element in driver.find_elements(By.XPATH, "//div[@class='yuRUbf']//a")]
    links.extend(linkedin_urls)  # Append the URLs to the links list
    # Wait for a short interval before the next iteration
    sleep(5)
    
split_links= [links[x:x+30] for x in range(0,len(links), 30)]


lock=Lock()

# ID=20230000    
def linkedin_scrape_function(chunk,lock):
    
    for link in chunk:
        
        lock.acquire()
        driver.get(link)
        sleep(6)
        sel= Selector(text=driver.page_source)
        
        name = sel.xpath('//*[starts-with(@class,"text-heading-xlarge inline t-24 v-align-middle break-words")]/text()').extract_first()
        job_title = sel.xpath('//*[starts-with(@class,"text-body-medium break-words")]/text()').extract_first()
        if name and job_title:
            name = name.strip()
            job_title = job_title.strip()
        skill_url = []
        exp_url=[]
        href_skill = [element.get_attribute("href") for element in driver.find_elements(By.XPATH, "//div[@class='pvs-list__footer-wrapper']//a[contains(@id,'skills')]")]
        skill_url.extend(href_skill)
        href_exp = [element.get_attribute("href") for element in driver.find_elements(By.XPATH, "//div[@class='pvs-list__footer-wrapper']//a[contains(@id,'experiences')]")]
        exp_url.extend(href_exp)
        
        candidate_edu=education(link)

        candidate_exp=experience(link,exp_url)
        # print(href_skill)
        candidate_skills=skills(link,skill_url)

#         ID=ID+1
        # print('\n')
        # print('Name: '+name)
        # print('Job_Title: '+job_title)
        # print(candidate_edu)
        # print(candidate_exp)
        # print(candidate_skills)
        
        data={'ID':1,
            'Name':name,
            'Job_Title':job_title,
            'education':candidate_edu,
            'expereince':candidate_exp,
            'Skills':candidate_skills}

        profile_data.append(data)
        lock.release()
    return profile_data

threadList = []
for i in range(len(split_links)):
    t = threading.Thread(target=linkedin_scrape_function, args=[split_links[i],lock])
#     multiprocessing.Process(tar)
    t.start()
    threadList.append(t)
print(threadList)

for eachThread in threadList:
    eachThread.join()
    
df= pd.DataFrame(profile_data)

df.to_excel("Linkedin_data_engineers.xlsx")

driver.quit()


# In[ ]:


split_links


# In[3]:


df


# In[ ]:




